[{"content":"","title":"","uri":"/about/about-me/"},{"content":"","title":"Education","uri":"/resume/education/"},{"content":"","title":"Experience","uri":"/resume/experience/"},{"content":"","title":"Honors \u0026 Awards","uri":"/resume/accomplishments/"},{"content":"In this episode, hosts Dr. Tanner Schrank and Sophie Neale delve deep into the world of Artificial Intelligence in healthcare with AI expert, Dr. Anne-Kathrin Kleine. From exploring how clinicians view AI to discussing its impact on mental health, Dr. Kleine paints a vivid picture of the technology’s potential. She expertly navigates us through both benefits and risks, providing invaluable insight for any medical student or health professional keen on the future of medicine. You won’t want to miss her insightful advice on embracing this technological frontier. Tune in for a glimpse into the future of healthcare.\nListen here or or everywhere else where podcasts are available.\n","title":"The Future of AI in Healthcare - Benefits, Risks, Doctors’ Attitudes, and Students’ Blind-spots","uri":"/talk/2023-podcast/"},{"content":" Summer School website Slides ","title":"A Glimpse Into the Future – How AI-Enabled Precision Psychiatry Tools Advance Mental Healthcare","uri":"/talk/2023-summerschool/"},{"content":" Slides ","title":"AI-ENABLED MEDICAL DEVICES • Progress • Potential • Challenges","uri":"/talk/2023-crosstalk/"},{"content":"PCI Registered Reports (PCI RR) is a dedicated community that reviews and recommends Registered Reports across different disciplines. The two-stage process involves reviewing study proposals and pre-accepting them (before research is conducted). Authors of recommended RRs have the option to publish in PCI RR-friendly journals. PCI RR aims to become a central hub for Registered Reports in all areas of research. For more info, please visit the PCI RR Website.\n","title":"Recommender for the Peer Community in Registered Reports (PCI RR)","uri":"/project/pcirr/"},{"content":"We recently conducted a study on the predictors of using AI-enabled mental health tools among psychology students and early practitioners. Here is a summary of our research:\nWhat we did: 206 psychology students and psychotherapists in training were presented with graphic depictions of two specific AI-enabled mental health tools and their functioning mechanisms. We measured various factors related to the acceptance and use of these tools. What we found: Perceived usefulness and social influence were strong predictors of intention to use the AI tools. Trust and perceived ease of use had limited impact on the intention to use the tools (especially the treatment recommendation tool). Cognitive technology readiness positively affected the intention to use one tool, while AI anxiety negatively impacted the intention to use both tools. What this means: Our findings highlight the importance of perceived usefulness and social influence in driving the adoption of AI technology in mental health care. Trust and perceived ease of use may not be strong factors influencing the intention to use AI tools in this context. Considering cognitive technology readiness can further promote the adoption of AI-enabled tools, while addressing AI anxiety is crucial in facilitating their utilization. This study provides valuable insights for psychology students and early practitioners considering the integration of AI tools into their professional practice, and opens avenues for future research in this field.\nLink to published paper ","title":"Attitudes Toward the Adoption of 2 Artificial Intelligence–Enabled Mental Health Tools Among Prospective Psychotherapists: Cross-sectional Study","uri":"/blog/2023-06-attitudes-toward-the-adoption-of-2-artificial-intelligenceenabled-mental-health-tools-among-prospective-psychotherapists-cross-sectional-study/"},{"content":" Conference website Slides ","title":"Predictors of the intention to use AI-enabled clinical decision support systems among healthcare practitioners.","uri":"/talk/2023-eawoutaut/"},{"content":"\n","title":"Effective leadership in research teams","uri":"/talk/2023-leadership/"},{"content":"\n","title":"Advancing Mental Health Care with AI-Enabled Precision Psychiatry Tools: A Patent Review","uri":"/talk/2023-patents/"},{"content":"\n","title":"Thriving at work","uri":"/talk/2023-thriving/"},{"content":" View the pre-print here\n","title":"Advancing Mental Health Care with AI-Enabled Precision Psychiatry Tools: A Patent Review","uri":"/blog/2023-02-advancing-mental-health-care-with-ai-enabled-precision-psychiatry-tools-a-patent-review/"},{"content":" Slides Worksheet Download here\n","title":"Körperliche Fitness – Sport als Hilfsmittel zur Stressbewältigung im Arbeitsalltag","uri":"/talk/2022-sports/"},{"content":" Slides ","title":"R and Quarto Workshop","uri":"/talk/2022-quartoteam/"},{"content":"** Contact page don’t contain a body, just the front matter above. See form.html in the layouts folder\nFormspree requires a (free) account and new form to be set up. The link is made on the final published url in the field: Restrict to Domain. It is possible to register up to 2 emails free and you can select which one you want the forms to go to within Formspree in the Settings tab. **\n","title":"Contact","uri":"/contact/"},{"content":"Please contact me for more information.\n","title":"Feedback reports for entrepreneurs using shiny and golem","uri":"/project/feedback-rep/"},{"content":"Thriving at work has been defined as employees’ joint sense of vitality and learning. Based on the socially embedded model of thriving at work, we examine several competing operationalizations of thriving at work. We hypothesize effects of (a) composite thriving, (b) separate vitality and learning scores, and (c) the interaction between vitality and learning, and we explore effects of (d) the congruence between vitality and learning on self-rated physical and mental health.\nWhat we found: Data came from n = 1,064 employees who participated in a four-wave study with one-month time lags. Results of multilevel linear and polynomial regression analyses showed that composite thriving was positively related to physical health, and composite thriving and vitality were positively related to mental health at the within-person level. We found no support for interaction or congruence effects. The findings provide limited support for the assumed beneficial health effects of thriving on employees’ health.\nWhat this means: Implications for theory development include the need to revise the role of vitality and learning as predictors of physical and mental health in the model of thriving at work.\nLink to published paper ","title":"Thriving at work: An investigation of the independent and joint effects of vitality and learning on employee health","uri":"/blog/2022-08-thriving-at-work-an-investigation-of-the-independent-and-joint-effects-of-vitality-and-learning-on-employee-health/"},{"content":"Shedding light on social-cognitive resources that mitigate master students’ experience of dysfunctional career-related worry before graduation.\nWhat we did: Based on the career self-management model (CSM; Lent \u0026 Brown, 2013), we investigated concurrent and time-lagged direct and mediated relationships between career planning, career-related self-efficacy, and career-related worry among a sample of 482 students shortly before graduation.\nWhat we found: Using data collected at three time points, a negative relationship was found between career planning (T1) and career-related worry (T3) via career-related self-efficacy (T2). Our findings shed light on the role of career planning and career-related self-efficacy as malleable social-cognitive resources that diminish dysfunctional thinking before graduation in sequential order. The findings imply that career planning and career-related self-efficacy are relevant predictors of affective states and can be incorporated into the CSM.\nLink to published paper ","title":"Career planning and self-efficacy as predictors of students’ career-related worry","uri":"/blog/2022-06-career-planning-and-self-efficacy-as-predictors-of-students-career-related-worry/"},{"content":" Career exploration refers to the exploration of the environment and the self with the aim of gathering career-related information.\nWhat we did: On the basis of Lent and Brown’s (2013) model of career self-management (CSM), the current meta-analysis examined the antecedents and outcomes of career exploration among college students (K = 109, N = 34,969 students).\nWhat we found: We found support for the applicability of the CSM model to the context of students’ career exploration. Specifically, positive associations were found for the association of the three core person-cognitive variables self-efficacy for career exploration and decision-making (rc = 0.52), outcome expectations (rc = 0.31), and career-exploratory goals (rc = 0.42) with career exploration. Results of path analyses suggest that the effects of both self-efficacy and outcome expectations on career exploration are mediated by career-exploratory goals. Further, in line with the CSM model, career exploration was positively related to career-related support (rc = 0.33) and negatively related to barriers (rc = − 0.15). Moreover, career exploration was associated with important career-related outcomes, such as career decidedness (rc = 0.22), and perceived employability (rc = 0.35). Exploratory moderator analyses revealed that some relationships are influenced by sample (i.e., age, gender, cultural background) and measurement (e.g., publication date) characteristics.\nWhat this implies: The findings of this meta-analysis highlight several implications for the further development of the CSM model, future research on students’ career exploration, and career development practice.\nLink to published paper ","title":"Students' Career exploration: a meta-analysis","uri":"/blog/2022-06-students-career-exploration-a-meta-analysis/"},{"content":"According to transactional stress theory (TST), the extent to which entrepreneurs cope with errors by engaging in error damage control or ruminating about disengaging from their business goals depends on whether they interpret action errors as predominantly challenging or threatening.\nWhat we did: Using latent profile analysis (LPA), the current study investigates the existence of latent profiles of challenge and threat appraisal of entrepreneurial errors and their relationship with error damage control and rumination about business goal disengagement in a sample of 649 entrepreneurs.\nWhat we found: The results identify five appraisal profiles characterized by different challenge and threat appraisal intensities. The levels of error damage control and rumination about business goal disengagement differed between the profiles. Specifically, entrepreneurs high in challenge and threat appraisal showed higher levels of both forms of coping than those low in appraisal. Entrepreneurs falling into a high challenge and low threat appraisal profile exhibited the lowest level of business goal disengagement. Still, they showed similar levels of error damage control to those high on challenge and threat appraisal.\nWhat this means: Entrepreneurs may experience action errors as challenging and threatening simultaneously. Error management approaches may be used to foster an atmosphere in which errors are appraised as learning opportunities rather than threats.\n","title":"Entrepreneurs may experience errors as simultaneously challenging and threatening","uri":"/blog/2022-06-entrepreneurs-may-experience-errors-as-simultaneously-challenging-and-threatening/"},{"content":" Preparation 1) Install R Please make sure that you have a recent R version installed. You can check your R Version in the Console (in RStudio) by typing: R.version.string\nIf you are using an older R version, please install the newest R version.\n2) Install R Studio Go to the RStudio website and download the newest version if you are using an older version of RStudio (if in doubt, better update!). Simply follow the download instructions. The new version will be available next time you open RStudio.\n3) Install necessary packages You need several R packages for the workshop. Please make sure to install these packages before the workshop. The easiest way to do so is by running this code in R:\ncran_packs \u003c- c(\"learnr\", \"bookdown\", \"stargazer\", \"officer\", \"rticles\", \"webshot\", \"tidyverse\", \"remotes\", \"magick\", \"kableExtra\", \"vtable\", \"devtools\") install.packages(cran_packs, dependencies = TRUE) github_packs \u003c- c('rstudio/blogdown', 'haozhu233/kableExtra', 'crsh/papaja') remotes::install_github(github_packs, dependencies = TRUE) Once these packages are installed, you may use blogdown to install the Hugo package. Hugo is a static site generator that we will use to build blogdown paged. You may install Hugo using blogdown, like so:\nblogdown::install_hugo() You can check whether the installations were successful by running library(package_name). All installed packages must be loaded before we can use them during the workshop.\n4) Set up Git and GitHub To make the most out of the workshop, I highly encourage you to install Git and to create a GitHub account for version control. Follow the instructions from the Happy Git with R Book and the official GitHub guides:\nRegister a GitHub account\nInstall Git. Follow the instructions under Setting up Git (do not use the Desktop client). For Windows Users: Follow the installation instructions for Git here.\nOnce this is done, you are all set for the workshop! 🎉 5) Sign up on osf Many social scientists use osf to store their data or code. It is possible to integrate your GitHub repositories into osf. Please sign up on osf before the workshop.\n6) Install TeX Windows: MikTeX Mac: MacTeX Slides Day 1 Available upon request.\nDay 2 Available upon request.\nSchedule 🗓️ Dates: 24th \u0026 25th May 2022\n🕛 Timetable for Day 1: Unit When What 09:00-09:30 🖐 WALK-IN and final installation support Session #1 09:30-10:15 Introduction to R Markdown 10:15-10:30 🍵 BREAK Session #2 10:30-11:15 R Markdown - Text, code, reference management 11:15-11:30 🍵 BREAK Session #3 11:30-13:00 R Markdown - extended options and project organisation 13:00-13:40 🍔 LUNCH BREAK Session #4 13:40-15:00 R Markdown - building your own project 🕛 Timetable for Day 2: Unit When What 09:00-09:30 ❓ Q \u0026 A Session #1 09:30-10:15 Version control with Git \u0026 GitHub - the basics 10:15-10:30 🍵 BREAK Session #2 10:30-11:15 Version control with Git \u0026 GitHub - collaborating; Integration with osf 11:15-11:30 🍵 BREAK Session #3 11:30-13:00 Blogdown - build your own project websites 13:00-13:40 🍔 LUNCH BREAK Session #4 13:40-15:00 Other R (Markdown) tools, final Q \u0026 A ","title":"R Markdown Workshop","uri":"/talk/2022-kli/"},{"content":" Thriving at work refers to a positive psychological state characterized by a joint sense of vitality and learning.\nWhat we did: On the basis of Spreitzer and colleagues’ model, we present a comprehensive meta-analysis of antecedents and outcomes of thriving at work (K = 73 independent samples, N = 21,739 employees).\nWhat we found: Results showed that thriving at work is associated with individual characteristics, such as psychological capital (rc = .47), proactive personality (rc = .58), positive affect (rc = .52), and work engagement (rc = .64). Positive associations were also found between thriving at work and relational characteristics, including supportive coworker behavior (rc = .42), supportive leadership behavior (rc = .44), and perceived organizational support (rc = .63). Moreover, thriving at work is related to important employee outcomes, including health-related outcomes such as burnout (rc = −.53), attitudinal outcomes such as commitment (rc = .65), and performance-related outcomes such as task performance (rc = .35). The results of relative weights analyses suggest that thriving exhibits small, albeit incremental predictive validity above and beyond positive affect and work engagement, for task performance, job satisfaction, subjective health, and burnout.\nWhat this means: Overall, the findings of this meta-analysis support Spreitzer and colleagues’ model and underscore the importance of thriving in the work context.\nLink to published paper ","title":"Thriving at work: a meta-analysis","uri":"/blog/2020-06-thriving-at-work-a-meta-analysis/"},{"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","title":"LICENSE: CC-BY-SA","uri":"/license/"},{"content":" It is great to have you visiting my website! As of June 2022, I am a data scientist and researcher in the Human-AI-Interaction Group at the University of Munich (LMU). Before this, I completed my Ph.D. in Organisational Behavior at the University of Groningen. During this time I developed a passion for data science and data-based storytelling. In my current role, I explore the opportunities and challenges of using artificial intelligence and machine learning in mental healthcare. Furthermore, I provide teaching and guidance for individuals and groups to use and collaborate with R for data analysis and reporting purposes.\n","title":"","uri":"/about/header/"},{"content":"** index doesn’t contain a body, just front matter above. See about/list.html in the layouts folder **\n","title":"","uri":"/about/sidebar/"},{"content":"","title":"","uri":"/publication/template/"},{"content":"** index doesn’t contain a body, just front matter above. See about/list.html in the layouts folder **\n","title":"Lately","uri":"/about/main/"},{"content":" I used natural language processing (NLP) to analyse texts entrepreneurs wrote about the errors that happened in their business over the past two weeks. Entrepreneurs in the sample were from India and the Netherlands, operating in various industries.\nData preparation Loading standard libraries and source custom functions library(kableExtra) library(tidyverse) library(expss) library(lattice) source(\"R/custom_functions.R\") Reading in data and preprocessing I prepared the data for reliability assessment and correlation analysis.\ndat \u003c- read_csv(\"data/dat.csv\") evdes \u003c- dat$t1evdes_ comp_dat \u003c- dat %\u003e% dplyr::select(matches(\"t1emotions|jobstr|jobsa|t1threat|gender|age|found|t1occ$|lang|edu|max_sev\")) alph_dat \u003c- dat %\u003e% dplyr::select(matches(\"t1emotions|jobstr|t1threat\")) comp_split \u003c- comp_dat %\u003e% split.default(sub(\"_.*\", \"\", names(comp_dat))) alph_split \u003c- alph_dat %\u003e% split.default(sub(\"_.*\", \"\", names(alph_dat))) comp \u003c- map(comp_split, ~ multicon::composite(.x, nomiss = 0.8), data = .x) %\u003e% as.data.frame(.) alph \u003c- map(alph_split, ~ psych::alpha(.x, check.keys=TRUE), data = .x) %\u003e% map(~ .x$total) alph_df \u003c- do.call(\"rbind\", alph) %\u003e% round(., 2) Overview of sample characteristics Age of the entrepreneurs in the sample: Educational level: Gender: Language: Current occupation: Whether they were involved in founding the business: Reliabilities for multi-item variables: alph_df %\u003e% DT::datatable( extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('excel', \"csv\"), pageLength = 20)) Correlation table: cor \u003c- round(cor(comp, use=\"pairwise.complete.obs\"), 2) corstar_select \u003c- data.frame(corstars(comp, removeTriangle = \"none\", result=\"none\")) corstar_select %\u003e% DT::datatable( extensions = 'Buttons', options = list(dom = 'Bfrtip', buttons = c('excel', \"csv\"), pageLength = 10, lengthMenu = c(25, 50, 75, 94))) Text mining First, I created an annotated data frame from the text data.\nlibrary(udpipe) ud_model \u003c- udpipe_download_model(language = \"english\") ud_model \u003c- udpipe_load_model(ud_model$file_model) x \u003c- udpipe_annotate(ud_model, x = evdes) x \u003c- as.data.frame(x) Next, I selected nouns and adjectives from the text data frame and removed duplicate entries.\nlibrary(tm) stats \u003c- subset(x, upos %in% c(\"NOUN\", \"ADJ\")) stats2 \u003c- stats %\u003e% dplyr::group_by(doc_id) %\u003e% dplyr::mutate(sentences = paste0(token, collapse = \" \")) evdes_nouns \u003c- stats2[!(duplicated(stats2$sentences) | duplicated(stats2$sentences)),] %\u003e% dplyr::select(sentences) evdes \u003c- evdes_nouns$sentences evdes_1 \u003c- VectorSource(evdes) TextDoc \u003c- Corpus(evdes_1) Cleaning text data I cleaned the data by removing unnecessary white space and converting special characters into white space. I also transformed all letters to lower case, removed numbers, stop words (e.g., and, or…), and punctuation. Finally, I lemmatized the remaining words (see here for more information on the lemmatization function.)\nlibrary(textstem) #Replacing \"/\", \"@\" and \"|\" with space toSpace \u003c- content_transformer(function(x, pattern ) gsub(pattern, \" \", x)) TextDoc \u003c- tm_map(TextDoc, toSpace, \"/\") TextDoc \u003c- tm_map(TextDoc, toSpace, \"@\") TextDoc \u003c- tm_map(TextDoc, toSpace, \"\\\\|\") TextDoc \u003c- tm_map(TextDoc, toSpace, \"\\\\|\") # Convert the text to lower case TextDoc \u003c- tm_map(TextDoc, content_transformer(tolower)) # Remove numbers TextDoc \u003c- tm_map(TextDoc, removeNumbers) # Remove english common stopwords TextDoc \u003c- tm_map(TextDoc, removeWords, stopwords(\"english\")) # Remove punctuations TextDoc \u003c- tm_map(TextDoc, removePunctuation) # Eliminate extra white spaces TextDoc \u003c- tm_map(TextDoc, stripWhitespace) # Text stemming - which reduces words to their root form #TextDoc \u003c- tm_map(TextDoc, stemDocument) TextDoc \u003c- tm_map(TextDoc, lemmatize_strings) Building the document martix # Build a term-document matrix TextDoc_tdm \u003c- TermDocumentMatrix(TextDoc) TextDoc_dtm \u003c- DocumentTermMatrix(TextDoc) TextDoc_tdm \u003c- removeSparseTerms(TextDoc_tdm, .99) TextDoc_dtm \u003c- removeSparseTerms(TextDoc_dtm, .99) dtm_m \u003c- as.matrix(TextDoc_tdm) # Sort by descearing value of frequency dtm_v \u003c- sort(rowSums(dtm_m),decreasing=TRUE) dtm_d \u003c- data.frame(word = names(dtm_v),freq=dtm_v) # Display the top 5 most frequent words head(dtm_d, 5) word freq product product 22 client client 21 business business 16 time time 16 customer customer 12 A word cloud of the most common nouns and adjectives library(wordcloud) #generate word cloud set.seed(1234) wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5, scale=c(3,.4), max.words=100, random.order=FALSE, rot.per=0.40, colors=brewer.pal(8, \"Dark2\")) The most frequently occurring nouns and adjectives stats \u003c- subset(x, upos %in% c(\"NOUN\", \"ADJ\")) stats \u003c- txt_freq(x = stats$lemma) dtm_d$word \u003c- factor(dtm_d$word, levels = rev(dtm_d$word)) dtm_head \u003c- head(dtm_d, 22) barchart(word ~ freq, data = dtm_head, col = \"cadetblue\", main = \"Most occurring nouns and adjectives\", xlab = \"Freq\") Showing connections between words I created a cluster dendogram to show connections between words.\nlibrary(tidytext) dtm_top \u003c- removeSparseTerms(TextDoc_tdm, sparse = .97) TextDoc_tdm_m \u003c- as.matrix(dtm_top) distance \u003c- dist(TextDoc_tdm_m, method = \"euclidean\") fit \u003c- hclust(distance, method = \"complete\") plot(fit) Hmm.. not super informative. What about trying to find topics in the texts?\nTopic modeling I started with 10 topics… library(topicmodels) rowTotals \u003c- apply(TextDoc_dtm , 1, sum) TextDoc_dtm \u003c- TextDoc_dtm[rowTotals\u003e 0, ] # set a seed so that the output of the model is predictable ap_lda \u003c- LDA(TextDoc_dtm, k = 10, control = list(seed = 1234)) ap_lda A LDA_VEM topic model with 10 topics. #\u003e A LDA_VEM topic model with 2 topics. ap_topics \u003c- tidy(ap_lda, matrix = \"beta\") ap_top_terms \u003c- ap_topics %\u003e% group_by(topic) %\u003e% slice_max(beta, n = 4) %\u003e% ungroup() %\u003e% arrange(topic, -beta) ap_top_terms %\u003e% mutate(term = reorder_within(term, beta, topic)) %\u003e% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = \"free\") + scale_y_reordered() The visualisation displays the per-topic-per-word probabilities (called beta). For each word combination, the model computes the probability of that term being generated from that topic. For example, the most common words in topic 1 include “part”, “lack”, and “fund”. Maybe sth. related to capital and funding? Topic five revolves around issues with customers and service. The usefulness of the topic modeling always depends on the text data. And finding the right number of topics to extract is an iterative approach.\nHere’s a solution for 6 topics: # set a seed so that the output of the model is predictable ap_lda \u003c- LDA(TextDoc_dtm, k = 6, control = list(seed = 1234)) #\u003e A LDA_VEM topic model with 2 topics. ap_topics \u003c- tidy(ap_lda, matrix = \"beta\") ap_top_terms \u003c- ap_topics %\u003e% group_by(topic) %\u003e% slice_max(beta, n = 5) %\u003e% ungroup() %\u003e% arrange(topic, -beta) ap_top_terms %\u003e% mutate(term = reorder_within(term, beta, topic)) %\u003e% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = \"free\") + scale_y_reordered() See here for more information on topic modeling.\nWe could also consider examining the words with the greatest difference in beta between two topics. This can be estimated based on the log ratio of the two. We can filter for relatively common words to make the example more concrete. Here, we filter for words with a beta greater than 0.001.\nlibrary(tidyr) beta_wide \u003c- ap_topics %\u003e% mutate(topic = paste0(\"topic\", topic)) %\u003e% pivot_wider(names_from = topic, values_from = beta) %\u003e% filter(topic1 \u003e .001 | topic2 \u003e .001) %\u003e% mutate(log_ratio = log2(topic2 / topic1)) beta_wide # A tibble: 59 × 8 term topic1 topic2 topic3 topic4 topic5 topic6 log_ratio \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e 1 financial 1.24e- 2 9.01e-60 2.00e- 2 7.09e-23 1.12e-13 4.47e-43 -190. 2 month 7.43e- 2 2.18e-29 1.92e-48 1.20e-78 1.14e-58 2.59e-45 -91.5 3 revenue 2.27e- 2 4.89e-69 1.78e-75 2.45e-63 1.29e- 2 3.24e-74 -221. 4 support 3.72e- 2 6.41e-25 3.27e-68 1.18e-21 3.88e-31 4.42e-25 -75.6 5 time 2.58e- 2 3.43e- 2 4.79e-14 1.23e- 1 8.13e-23 2.26e- 3 0.408 6 unable 2.48e- 2 6.61e-18 3.88e-28 4.38e-52 5.72e-10 1.53e-61 -51.7 7 company 4.81e-47 6.45e- 2 2.68e-42 3.58e-39 1.13e-71 2.67e-35 150. 8 contact 4.77e-69 2.58e- 2 1.33e-74 1.39e-71 3.81e-30 4.05e-71 222. 9 day 5.11e-59 5.02e- 2 1.01e- 2 4.96e-14 6.97e-36 1.15e- 3 189. 10 job 9.47e-69 5.16e- 2 3.06e-75 5.72e-71 2.03e-74 6.21e-25 222. # … with 49 more rows beta_wide %\u003e% group_by(direction = log_ratio \u003e 0) %\u003e% slice_max(abs(log_ratio), n = 10) %\u003e% ungroup() %\u003e% mutate(term = reorder(term, log_ratio)) %\u003e% ggplot(aes(log_ratio, term)) + geom_col() + labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL) ","title":"Using NLP for examining entrepreneurs' business errors","uri":"/project/events/"},{"content":"We used natural language processing (NLP) tools in R to analyse newspaper articles about revenge bedtime procrastination. We preselected articles that contain the keyword revenge bedtime procrastination in the title.\nData preparation Loading standard libraries and source custom functions library(kableExtra) library(tidyverse) library(expss) library(lattice) source(\"R/custom_functions.R\") Reading in data and preprocessing dat \u003c- readxl::read_excel(\"data/newsdata_selected.xlsx\") content \u003c- dat$Content Text mining First, we created an annotated data frame from the text data.\nlibrary(udpipe) ud_model \u003c- udpipe_download_model(language = \"english\") The Universal Dependencies (UD) models contain 94 models of 61 languages, each consisting of a tokenizer, tagger, lemmatizer and dependency parser, all trained using the UD data. See here for more information. We downloaded the English language Universal Dependencies (UD) model to use it for our text data.\nud_model \u003c- udpipe_load_model(ud_model$file_model) x \u003c- udpipe_annotate(ud_model, x = content) x \u003c- as.data.frame(x) Next, we selected nouns and adjectives from the text data frame and removed duplicate entries.\nlibrary(tm) stats \u003c- subset(x, upos %in% c(\"NOUN\", \"ADJ\")) stats2 \u003c- stats %\u003e% dplyr::group_by(doc_id) %\u003e% dplyr::mutate(sentences = paste0(token, collapse = \" \")) text_nouns \u003c- stats2[!(duplicated(stats2$sentences) | duplicated(stats2$sentences)),] %\u003e% dplyr::select(sentences) evdes \u003c- text_nouns$sentences evdes_1 \u003c- VectorSource(evdes) TextDoc \u003c- Corpus(evdes_1) Cleaning text data Next, we cleaned the text data. Specifically, we removed unnecessary white space and converted special characters into white space. We also transformed letters to lower case, removed numbers, stop words (e.g., and, or…), and punctuation. Finally, used lemmatization for remaining words (see here for more information on lemmatization.)\nlibrary(textstem) #Replacing \"/\", \"@\" and \"|\" with space toSpace \u003c- content_transformer(function(x, pattern ) gsub(pattern, \" \", x)) TextDoc \u003c- tm_map(TextDoc, toSpace, \"/\") TextDoc \u003c- tm_map(TextDoc, toSpace, \"@\") TextDoc \u003c- tm_map(TextDoc, toSpace, \"\\\\|\") TextDoc \u003c- tm_map(TextDoc, toSpace, \"\\\\|\") # Convert the text to lower case TextDoc \u003c- tm_map(TextDoc, content_transformer(tolower)) # Remove numbers TextDoc \u003c- tm_map(TextDoc, removeNumbers) # Remove english common stopwords TextDoc \u003c- tm_map(TextDoc, removeWords, stopwords(\"english\")) # Remove punctuations TextDoc \u003c- tm_map(TextDoc, removePunctuation) # Eliminate extra white spaces TextDoc \u003c- tm_map(TextDoc, stripWhitespace) # Text stemming - which reduces words to their root form #TextDoc \u003c- tm_map(TextDoc, stemDocument) TextDoc \u003c- tm_map(TextDoc, lemmatize_strings) We also removed a couple of words that are not very informative for understanding the phenomenon, including words relating to time or abstract words like “thing”, “many”, “medium”, …\nTextDoc \u003c- tm_map(TextDoc, removeWords, c(\"sleep\", \"day\", \"time\", \"hour\", \"night\",\"long\", \"late\", \"year\", \"minute\", \"week\", \"daytime\", \"people\", \"thing\", \"term\", \"way\", \"reason\", \"expert\", \"effect\", \"part\", \"life\", \"activity\", \"person\", \"self\", \"consequence\", \"read\", \"amount\", \"problem\", \"behaviour\", \"behavior\", \"concept\", \"many\", \"medium\", \"important\", \"much\", \"enough\", \"next\", \"enough\", \"important\", \"good\", \"little\", \"right\", \"high\", \"hard\", \"new\", \"even\", \"bad\", \"close\")) Additionally, we create a text document with our keywords (“revenge”, “bedtime”, “procrastination”) removed.\nTextDoc_wc \u003c- tm_map(TextDoc, removeWords, c(\"revenge\", \"bedtime\", \"sleep\", \"procrastination\", \"day\")) We also removed words that appear in less than 90 percent of all documents.\n# Build a term-document matrix TextDoc_tdm \u003c- TermDocumentMatrix(TextDoc) TextDoc_dtm \u003c- DocumentTermMatrix(TextDoc) TextDoc_tdm \u003c- removeSparseTerms(TextDoc_tdm, .90) TextDoc_dtm \u003c- removeSparseTerms(TextDoc_dtm, .90) TextDoc_tdm_wc \u003c- TermDocumentMatrix(TextDoc_wc) TextDoc_dtm_wc \u003c- DocumentTermMatrix(TextDoc_wc) TextDoc_tdm_wc \u003c- removeSparseTerms(TextDoc_tdm_wc, .90) TextDoc_dtm_wc \u003c- removeSparseTerms(TextDoc_dtm_wc, .90) We sorted the results by the number the terms appear in the texts (decreasing).\ndtm_m \u003c- as.matrix(TextDoc_tdm) # Sort by descearing value of frequency dtm_v \u003c- sort(rowSums(dtm_m),decreasing=TRUE) dtm_d \u003c- data.frame(word = names(dtm_v),freq=dtm_v) # Display the top 5 most frequent words dtm_m_wc \u003c- as.matrix(TextDoc_tdm_wc) # Sort by descearing value of frequency dtm_v_wc \u003c- sort(rowSums(dtm_m_wc),decreasing=TRUE) dtm_d_wc \u003c- data.frame(word = names(dtm_v_wc),freq=dtm_v_wc) # Display the top 5 most frequent words # As was to be expected, the most frequently appearing words in the original documents are our keywords.\nhead(dtm_d, 5) word freq procrastination procrastination 47 bedtime bedtime 44 bed bed 36 revenge revenge 33 work work 29 But if the keywords are removed, the ten most frequent words are:\nhead(dtm_d_wc, 10) word freq bed bed 36 work work 29 phone phone 20 control control 18 health health 17 schedule schedule 17 light light 13 social social 13 pandemic pandemic 12 study study 11 A word cloud of the most common nouns and adjectives library(wordcloud) #generate word cloud set.seed(1234) wordcloud(words = dtm_d_wc$word, freq = dtm_d$freq, min.freq = 5, scale=c(3,.4), max.words=100, random.order=FALSE, rot.per=0.40, colors=brewer.pal(8, \"Dark2\")) The most frequently occurring nouns and adjectives stats \u003c- subset(x, upos %in% c(\"NOUN\", \"ADJ\")) stats \u003c- txt_freq(x = stats$lemma) dtm_d_wc$word \u003c- factor(dtm_d_wc$word, levels = rev(dtm_d_wc$word)) dtm_head \u003c- head(dtm_d_wc, 22) barchart(word ~ freq, data = dtm_head, col = \"cadetblue\", main = \"Most occurring nouns and adjectives\", xlab = \"Freq\") Showing connections between words We created a cluster dendogram to show connections between words, with a sparsity threshold of 60 percent.\nlibrary(tidytext) dtm_top \u003c- removeSparseTerms(TextDoc_tdm, sparse = .60) TextDoc_tdm_m \u003c- as.matrix(dtm_top) distance \u003c- dist(TextDoc_tdm_m, method = \"euclidean\") fit \u003c- hclust(distance, method = \"complete\") plot(fit) Topic modeling We started with 10 topics We again selected the text matrix without the keywords (“revenge”, “bedtime”, and “procrastination”)\nlibrary(topicmodels) rowTotals \u003c- apply(TextDoc_dtm_wc , 1, sum) TextDoc_dtm \u003c- TextDoc_dtm_wc[rowTotals\u003e 3, ] # set a seed so that the output of the model is predictable ap_lda \u003c- LDA(TextDoc_dtm, k = 10, control = list(seed = 1234)) ap_lda A LDA_VEM topic model with 10 topics. #\u003e A LDA_VEM topic model with 2 topics. ap_topics \u003c- tidy(ap_lda, matrix = \"beta\") ap_top_terms \u003c- ap_topics %\u003e% group_by(topic) %\u003e% slice_max(beta, n = 4) %\u003e% ungroup() %\u003e% arrange(topic, -beta) ap_top_terms %\u003e% mutate(term = reorder_within(term, beta, topic)) %\u003e% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = \"free\") + scale_y_reordered() The visualisation displays the per-topic-per-word probabilities (called beta). For each word combination, the model computes the probability of that term being generated from that topic. For example, the most common words in topic 1 include “part”, “lack”, and “fund”. Maybe sth. related to capital and funding? Topic five revolves around issues with customers and service. The usefulness of the topic modeling always depends on the text data. And finding the right number of topics to extract is an iterative approach.\nHere’s a solution for 8 topics: # set a seed so that the output of the model is predictable ap_lda \u003c- LDA(TextDoc_dtm, k = 8, control = list(seed = 1234)) #\u003e A LDA_VEM topic model with 2 topics. ap_topics \u003c- tidy(ap_lda, matrix = \"beta\") ap_top_terms \u003c- ap_topics %\u003e% group_by(topic) %\u003e% slice_max(beta, n = 5) %\u003e% ungroup() %\u003e% arrange(topic, -beta) ap_top_terms %\u003e% mutate(term = reorder_within(term, beta, topic)) %\u003e% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = \"free\") + scale_y_reordered() Here’s a solution for 6 topics: # set a seed so that the output of the model is predictable ap_lda \u003c- LDA(TextDoc_dtm, k = 6, control = list(seed = 1234)) #\u003e A LDA_VEM topic model with 2 topics. ap_topics \u003c- tidy(ap_lda, matrix = \"beta\") ap_top_terms \u003c- ap_topics %\u003e% group_by(topic) %\u003e% slice_max(beta, n = 5) %\u003e% ungroup() %\u003e% arrange(topic, -beta) ap_top_terms %\u003e% mutate(term = reorder_within(term, beta, topic)) %\u003e% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = \"free\") + scale_y_reordered() See here for more information on topic modeling.\nWe could also consider examining the words with the greatest difference in beta between two topics. This can be estimated based on the log ratio of the two. We can filter for relatively common words to make the example more concrete. Here, we filter for words with a beta greater than 0.005.\nlibrary(tidyr) beta_wide \u003c- ap_topics %\u003e% mutate(topic = paste0(\"topic\", topic)) %\u003e% pivot_wider(names_from = topic, values_from = beta) %\u003e% filter(topic1 \u003e .005 | topic2 \u003e .005) %\u003e% mutate(log_ratio = log2(topic2 / topic1)) beta_wide # A tibble: 167 × 8 term topic1 topic2 topic3 topic4 topic5 topic6 log_ratio \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e 1 alarm 0.00904 6.55e- 75 5.20e- 3 8.16e-242 4.63e-104 1.31e-74 -240. 2 balance 0.00904 2.67e-104 2.07e-104 2.80e-248 2.46e-104 9.48e-75 -337. 3 bed 0.0422 5.92e- 2 1.32e- 2 1.89e- 2 3.67e- 2 2.47e-74 0.489 4 binge 0.00602 9.51e- 75 8.36e- 75 6.29e- 3 5.69e- 3 1.89e-74 -239. 5 brain 0.0120 1.18e- 2 2.38e- 7 6.29e- 3 1.14e- 2 1.18e- 2 -0.0258 6 busy 0.00602 3.72e- 44 5.13e- 3 1.26e- 2 8.49e- 5 1.95e-74 -137. 7 chinese 0.00904 3.72e- 44 2.59e-104 1.52e-247 3.07e-104 1.18e- 2 -137. 8 clock 0.00602 5.92e- 3 1.04e- 2 1.43e-243 6.53e- 75 1.35e-74 -0.0258 9 concern 0.00301 5.92e- 3 3.15e-104 5.91e-240 5.66e- 75 1.17e-74 0.974 10 control 0.0151 5.92e- 3 1.82e- 2 6.29e- 3 1.99e- 2 4.71e- 2 -1.35 # … with 157 more rows beta_wide %\u003e% group_by(direction = log_ratio \u003e 0) %\u003e% slice_max(abs(log_ratio), n = 10) %\u003e% ungroup() %\u003e% mutate(term = reorder(term, log_ratio)) %\u003e% ggplot(aes(log_ratio, term)) + geom_col(fill = \"#b0157a\") + labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL) Sentiment analysis Sentiment Analysis is a process of extracting opinions that have different scores like positive, negative or neutral. Based on sentiment analysis, we can find out the nature of opinion or sentences in the newspaper articles.\nlibrary(tidytext) library(textdata) nrc_joy \u003c- get_sentiments(\"nrc\") %\u003e% filter(sentiment == \"joy\") joyful_aspects \u003c- dtm_d_wc %\u003e% inner_join(nrc_joy) %\u003e% count(word, sort = TRUE) A wordcloud of the joyful and pleasant aspects of revenge bedtime procrastination.\nlibrary(wordcloud) #generate word cloud set.seed(1234) wordcloud(words = joyful_aspects$word, freq = dtm_d$freq, min.freq = 5, scale=c(3,.4), max.words=100, random.order=FALSE, rot.per=0.40, colors=brewer.pal(8, \"Dark2\")) # Somehow, \"work\" gets an extremely high positive value. We remove the word for the sentiment analysis. dtm_d_wc \u003c- dtm_d_wc[- grep(\"work\", dtm_d_wc$word),] rbc_sentiment \u003c- dtm_d_wc %\u003e% inner_join(get_sentiments(\"bing\")) %\u003e% pivot_wider(names_from = sentiment, values_from = freq, values_fill = 0) %\u003e% mutate(sentiment = positive - negative) Now, we can plot the frequency of positive and negative words across all articles.\nrbc_sentiment$index \u003c- seq.int(nrow(rbc_sentiment)) ggplot(rbc_sentiment, aes(index, sentiment)) + geom_col(show.legend = FALSE) We see that negative words appear more frequently in the articles.\nFinally, we take a look at the particulalry negative and positive words (with sentiment values qual to or greater than three):\nrbc_sentiment_strong \u003c- rbc_sentiment %\u003e% filter(sentiment \u003e 3 | sentiment \u003c -3) ggplot(rbc_sentiment_strong, aes(word, sentiment)) + geom_col(show.legend = FALSE, fill = \"#4e1391\") ","title":"Using NLP for examining newspaper articles about revenge bedtime procrastination","uri":"/project/revenge/"}]
